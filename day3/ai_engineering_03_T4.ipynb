{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_6-jdBOXowG"
      },
      "source": [
        "**注意事項**\n",
        "\n",
        "このノートブックは、GPU:「T4」に対応させたものです。\n",
        "「L4」版のノートブックとはモデル等が異なるため、生成される内容が異なることが考えられます。\n",
        "\n",
        "生成される内容と、ノートブックに記載されている説明が一致しない場合があることをご了承ください。\n",
        "\n",
        "生成内容とノートブックの説明をよく見比べ、適宜読み替えながら演習を進めてみてください。\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 演習の方針\n",
        "\n",
        "1. **ベースラインモデル評価**  \n",
        "   素のモデルで回答を生成し、講義内容との整合性の低さを観察します。これにより、特別な学習なしでのモデルの限界を確認します。\n",
        "\n",
        "2. **文字起こしデータの活用**  \n",
        "   講義の文字起こしデータを導入し、モデルが講義内容を参照した回答を生成する傾向を観察します。ただし、Retrieval（情報検索）精度の限界から結果は不安定になる可能性があります。\n",
        "\n",
        "3. **チャンク化の導入**  \n",
        "   文字起こしデータをチャンク（小単位）に分割し、より安定して関連コンテンツを取得できるようにします。この段階では文脈理解にまだ課題があることを確認します。\n",
        "\n",
        "4. **Rerankの適用**  \n",
        "   検索結果のランク付けを導入し、より的確で安定した回答を目指します。\n",
        "\n",
        "5. **応用改善手法**  \n",
        "   文字起こしの品質向上のための編集技術や、メタデータの活用による性能向上手法を探ります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPI1pj4mFavt"
      },
      "source": [
        "## 扱う質問\n",
        "\n",
        "「Inference Time Scaling（推論時スケーリング）」に関する質問を取り扱います。これは以下の背景を持つトピックです。\n",
        "\n",
        "- 2024年8月発表の論文「Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters」で提唱された概念\n",
        "- OpenAIのGPT-o1（2024年9月リリース）で実用化され、注目を集めた比較的新しいアプローチ\n",
        "- 2024年度LLM講座の第4回講義でも取り上げられた重要テーマ\n",
        "\n",
        "## 扱うモデル\n",
        "\n",
        "「google/gemma-2-2b-jpn-it」を使用します。このモデルは、リリース時期の関係上、以下の特徴を持ちます。\n",
        "\n",
        "- 「Inference Time Scaling」の概念が広まる前に訓練されており、このトピックに関する知識を持たないと想定される\n",
        "- この特性を活かし、純粋なベースライン評価から各手法の効果を観察する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 演習環境の準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "vM50WAI7GXwC"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade transformers\n",
        "# !pip install google-colab-selenium\n",
        "# !pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "V2PStE0uqM03"
      },
      "outputs": [],
      "source": [
        "# 演習用のコンテンツを取得\n",
        "# !git clone https://github.com/matsuolab/lecture-ai-engineering.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: huggingface-cli <command> [<args>] login [-h] [--token TOKEN]\n",
            "                                                [--add-to-git-credential]\n",
            "huggingface-cli <command> [<args>] login: error: argument --token: expected one argument\n"
          ]
        }
      ],
      "source": [
        "# from dotenv import load_dotenv, find_dotenv\n",
        "# load_dotenv(find_dotenv())\n",
        "\n",
        "!huggingface-cli login --token $$HUGGINGFACE_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "zXo_kFASXlvp"
      },
      "outputs": [],
      "source": [
        "# # HuggingFace Login\n",
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "dZ_NUIftXwLc"
      },
      "outputs": [],
      "source": [
        "# CUDAが利用可能ならGPUを、それ以外ならCPUをデバイスとして設定\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "7eTgV8XBPA90"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "6tV9mO8oXoaM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it]\n"
          ]
        }
      ],
      "source": [
        "# モデル(Gemma2)の読み込み\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"google/gemma-2-2b-jpn-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=bnb_config,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "answers =[]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piTdVxTfGcc_"
      },
      "source": [
        "# 1. ベースラインモデル評価\n",
        "**まずはベースモデルがどの程度知識を持っているか確かめる**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "NBUZ3o6dhMlC"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"LLMにおけるInference Time Scalingとは？\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=False,\n",
        "    # temperature=0.6, # If do_sample=True\n",
        "    # top_p=0.9,  # If do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "4ZXyEnZ3lrBd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## LLMにおけるInference Time Scalingとは？\n",
            "\n",
            "**Inference Time Scaling**とは、Large Language Model (LLM) の推論時間（Inference Time）を効率的に調整する方法です。 \n",
            "\n",
            "**従来の推論時間:**\n",
            "\n",
            "* LLMの複雑な計算処理により、推論時間が非常に長くなる傾向があります。\n",
            "* リアルタイムでの応答が難しい、複雑な処理が必要な場合に大きな課題となります。\n",
            "\n",
            "**Inference Time Scalingの目的:**\n",
            "\n",
            "* **推論時間を短縮:**  より迅速な応答を実現し、ユーザーの満足度を高めます。\n",
            "* **コスト削減:**  推論時間を短縮することで、計算資源の消費量を削減できます。\n",
            "* **スケーラビリティ向上:**  大量のユーザーからのリクエストに対応できるよう、システムのスケーラビリティを高めます。\n",
            "\n",
            "**Inference Time Scalingを実現するための方法:**\n",
            "\n",
            "* **モデルの最適化:**  モデルの構造やパラメータを調整することで、推論時間を短縮できます。\n",
            "* **データの最適化:**  推論に必要となるデータのサイズや構造を最適化することで、処理時間を削減できます。\n",
            "* **ハードウェアの最適化:**  高速なCPUやGPU、メモリ\n"
          ]
        }
      ],
      "source": [
        "response = outputs[0][input_ids.shape[-1]:]\n",
        "print(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "answers.append(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSCNnRf9pJif"
      },
      "source": [
        "## 結果 (ベースモデル)\n",
        "\n",
        "「google/gemma-2-2b-jpn-it」は「Inference Time Scaling」について誤った知識を提示しました：\n",
        "* モデルは従来の「推論時間の短縮」という文脈でInference Time Scalingを解釈しており、これはLLM分野における最新の「Inference Time Scaling」概念（推論時計算資源の最適配分）とは異なる説明になります。\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4R-hiKNGyJd"
      },
      "source": [
        "# 2. 文字起こしデータの活用\n",
        "## 講義内容をソースとして活用 (RAG導入)\n",
        "\n",
        "モデルの回答の事実性を向上させるためにRetrieval Augmented Generation (RAG)技術を導入します：\n",
        "\n",
        "* **知識ソース**: LLM講座第4講における講師の発言内容\n",
        "* **目的**: モデルに「Inference Time Scaling」に関する正確な知識と文脈を提供し、事実に基づいた回答を促す\n",
        "\n",
        "**初期RAG実装（ベーシックアプローチ）**:\n",
        "* **ドキュメント処理**: 音声認識モデル(speech2text)で書き起こした生テキストをそのまま使用\n",
        "* **分割方法**: 「。」（句点）で区切られた文単位でテキストを分割\n",
        "* **検索手法**: シンプルな類似度ベースの検索でクエリに関連する文を抽出\n",
        "* **制約条件**: モデルの入力トークン制限に収まるよう関連文のみを選択"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "47GvcceyObAl"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "emb_model = SentenceTransformer(\"infly/inf-retriever-v1-1.5b\", trust_remote_code=True)\n",
        "# In case you want to reduce the maximum length:\n",
        "emb_model.max_seq_length = 4096"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "kPwggQfUS5yl"
      },
      "outputs": [],
      "source": [
        "with open(\"./data/LLM2024_day4_raw.txt\", \"r\") as f:\n",
        "# with open(\"/content/lecture-ai-engineering/day3/data/LLM2024_day4_raw.txt\", \"r\") as f:\n",
        "  raw_writedown = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "kxzKF6L2THIw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ドキュメントサイズ:  306\n",
            "ドキュメントの例: \n",
            " このDecodingにもいろんな方法があってグリーンDecodingだと単純に一番いいやつを選んでいく、一番確率が高いやつ選んでいくので、すごい単純ですけど、こういうトップKeyを取るとかトップ系を取るとかして、最後に一番いいやつを選ぶみたいなことをすると、これも結局計算をたくさんしてることになるわけですね\n"
          ]
        }
      ],
      "source": [
        "# ドキュメントを用意する。\n",
        "documents = [text.strip() for text in raw_writedown.split(\"。\")]\n",
        "print(\"ドキュメントサイズ: \", len(documents))\n",
        "print(\"ドキュメントの例: \\n\", documents[250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "nK4cYURzTHIx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[61.58913803100586, 65.9614028930664, 59.17729187011719, 53.388343811035156, 51.554901123046875, 58.392417907714844, 60.75017547607422, 54.14458465576172, 61.971519470214844, 62.857818603515625, 58.387691497802734, 59.10322189331055, 55.33921813964844, 57.35508728027344, 59.75800704956055, 58.144813537597656, 62.519622802734375, 57.498802185058594, 56.666446685791016, 59.71482849121094, 59.4350471496582, 64.46308135986328, 62.21357727050781, 60.33452606201172, 53.73405075073242, 64.26007843017578, 57.02006912231445, 62.448455810546875, 56.579444885253906, 56.528099060058594, 56.84428405761719, 54.060768127441406, 51.05217742919922, 51.92462158203125, 56.22425079345703, 56.69752502441406, 57.82189178466797, 57.06683349609375, 59.669166564941406, 59.15591049194336, 57.53609085083008, 52.950382232666016, 55.696495056152344, 54.76655960083008, 58.1320686340332, 61.39439392089844, 58.75428771972656, 61.9227409362793, 56.53096389770508, 58.76028060913086, 59.051002502441406, 53.05046844482422, 58.96952819824219, 54.51936721801758, 57.42409896850586, 59.27888107299805, 62.755977630615234, 56.343238830566406, 60.8527717590332, 62.38818359375, 61.107383728027344, 61.13219451904297, 64.14579010009766, 58.20071029663086, 57.470340728759766, 60.12662124633789, 55.964599609375, 64.83605194091797, 67.52084350585938, 60.01753234863281, 56.820533752441406, 61.08100891113281, 62.65396499633789, 53.12487030029297, 62.05617141723633, 63.25971984863281, 58.885860443115234, 56.64738464355469, 60.0349006652832, 58.890403747558594, 56.247825622558594, 58.51304626464844, 62.25620651245117, 58.36824035644531, 60.157249450683594, 53.599369049072266, 59.002750396728516, 52.59320831298828, 55.51559829711914, 63.137046813964844, 50.354278564453125, 53.59225845336914, 61.93791580200195, 60.36228942871094, 59.50149154663086, 58.82717514038086, 59.11053466796875, 57.0396842956543, 59.19733810424805, 60.690887451171875, 53.663124084472656, 58.90174102783203, 60.094261169433594, 59.61333084106445, 61.43379211425781, 61.45344924926758, 50.23605728149414, 52.51656723022461, 60.96503829956055, 59.19445037841797, 50.236053466796875, 52.38896560668945, 56.835960388183594, 60.01765823364258, 55.940711975097656, 53.32014083862305, 59.08241271972656, 53.98171615600586, 57.36327362060547, 62.80075454711914, 60.342918395996094, 52.573692321777344, 55.475807189941406, 52.88140106201172, 59.07683563232422, 50.95599365234375, 59.555564880371094, 51.662200927734375, 55.21050262451172, 61.095855712890625, 63.512611389160156, 60.96828079223633, 54.59373092651367, 58.94032669067383, 53.86066436767578, 53.5479621887207, 60.74045944213867, 62.58531951904297, 53.83000183105469, 58.307884216308594, 55.14292907714844, 52.69809341430664, 60.76683807373047, 53.25586700439453, 57.663543701171875, 59.58699035644531, 55.877418518066406, 55.25689697265625, 59.186439514160156, 54.06767272949219, 59.30153274536133, 59.03150939941406, 55.08674240112305, 56.25149154663086, 54.55509567260742, 58.244197845458984, 53.227054595947266, 56.82924270629883, 51.68625259399414, 52.341461181640625, 52.96391296386719, 58.18877410888672, 56.50699234008789, 52.35411071777344, 60.1298942565918, 51.671958923339844, 59.767425537109375, 60.723358154296875, 61.022239685058594, 60.29594421386719, 54.82716369628906, 55.46647262573242, 58.63345718383789, 57.462066650390625, 54.683773040771484, 61.056053161621094, 53.967655181884766, 62.32875442504883, 56.69070053100586, 64.29169464111328, 50.97770690917969, 58.142578125, 60.740081787109375, 51.516334533691406, 60.076290130615234, 55.62818145751953, 60.14632034301758, 54.01424026489258, 55.89878463745117, 58.9670295715332, 54.73461151123047, 59.076045989990234, 54.46656799316406, 59.478057861328125, 55.32378005981445, 60.27778244018555, 57.60865783691406, 56.05303192138672, 54.40284729003906, 57.743751525878906, 55.57329177856445, 58.2110595703125, 56.838321685791016, 55.53490447998047, 57.318199157714844, 61.35215759277344, 52.53401565551758, 58.90823745727539, 55.76476287841797, 58.62294387817383, 58.05729675292969, 52.15203094482422, 59.7059440612793, 58.453739166259766, 60.42371368408203, 59.22919464111328, 63.2206916809082, 65.18775177001953, 55.159664154052734, 58.178184509277344, 53.24652099609375, 60.72633743286133, 59.31788635253906, 64.23621368408203, 60.02073287963867, 57.24871063232422, 57.61821365356445, 55.753265380859375, 55.46892166137695, 53.408206939697266, 59.033607482910156, 60.612060546875, 59.20854187011719, 56.18899917602539, 65.13353729248047, 62.73472213745117, 53.500152587890625, 58.81696319580078, 53.47501754760742, 53.85403060913086, 61.768306732177734, 58.266014099121094, 64.80880737304688, 62.73647689819336, 61.91249084472656, 54.28956604003906, 59.3637809753418, 51.99961853027344, 64.30709075927734, 55.777191162109375, 55.768943786621094, 54.828857421875, 54.26502990722656, 51.08358383178711, 60.66939926147461, 53.2789421081543, 64.41972351074219, 59.604984283447266, 58.74079513549805, 51.23927688598633, 51.995243072509766, 53.492523193359375, 50.98735809326172, 53.30154800415039, 60.915077209472656, 57.451473236083984, 59.25986862182617, 58.0833854675293, 55.936668395996094, 57.5230827331543, 56.61652755737305, 55.11225509643555, 54.411773681640625, 57.482749938964844, 59.77004623413086, 50.49781036376953, 56.37862777709961, 58.30457305908203, 53.40733337402344, 57.84750747680664, 55.05374526977539, 58.498817443847656, 58.25198745727539, 59.38509750366211, 57.61931610107422, 51.886985778808594, 59.04962921142578, 52.76485824584961, 53.967689514160156, 55.65276336669922, 58.46554183959961, 54.17140197753906, 53.57320785522461, 55.119300842285156, 56.64287185668945, 58.71430587768555, 57.15926742553711, 56.75880432128906, 58.22709655761719, 54.204368591308594, 64.0544204711914, 57.66930389404297, 62.736446380615234, 66.03305053710938, 55.57224655151367, 50.88090896606445]]\n"
          ]
        }
      ],
      "source": [
        "# Retrievalの実行\n",
        "question = \"LLMにおけるInference Time Scalingとは？\"\n",
        "\n",
        "query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n",
        "document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "# 各ドキュメントの類似度スコア\n",
        "scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "print(scores.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "b_v8gx_tTHIx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "取得したドキュメント1: (Score: 67.52084350585938)\n",
            "Trasnformerの場合はスケール則がごめんなパラメータ数が横軸になってますけどこういうふうになると、LLMの場合の一掃にソヨンそうなのでそれぞれ計測と書くとこんなふうになりますよということで、Trasnformer以外のスケール則っていうのもあの研修をされて、深さについても検証してまして、これも他のモデルが何だったかちょっと忘れちゃったけどリスキーだったような気がしますけどそう変えたときにどういうふうな変化するかっていうのをこういった形でプロットするようなGENIACすることができます \n",
            "\n",
            "\n",
            "取得したドキュメント2: (Score: 66.03305053710938)\n",
            "1月に論文としてまして、スケーリングLLMthisTimeコンピュート口真理ちゃん日は増えてるっていうことで、良いらしいというふうに言われてます \n",
            "\n",
            "\n",
            "取得したドキュメント3: (Score: 65.9614028930664)\n",
            "あのスケールするっていうところではタイトルの通りなんですけど、ちょっとこれスケーリングPretrainingってなってるんですけれども、ちょっと最近はですね、このPretrainingだけではなくて、推論をスケールさせるというような話も出てきてましてせっかくなのでその最近の話題ということですレンジのスケーリングことでちょっとタイトル詐欺が入ってるんですけどPretrainingだけじゃない、スケーリングを扱うということでちょっと若干あのタイトル詐欺なんですけども、あの最近の話題ということで水土日のスケジュールについても話していきたいなと思っています \n",
            "\n",
            "\n",
            "取得したドキュメント4: (Score: 65.18775177001953)\n",
            "気にしながらっていうのの実例を出した方がわかりやすいと思うので、実際にこれ開発者じゃないので、あの結果を見て推論してるだけなんで嘘ついてるかもしれないですけど例えばLlama3の論文を持ってくると8Billon70Billon405Billonって層の数モデルDimension埋め込みの数次元ですね、フィードフォワードの次元、アテンションの数っていうのを、こういうふうにしたよっていうふうに言われてます \n",
            "\n",
            "\n",
            "取得したドキュメント5: (Score: 65.13353729248047)\n",
            "ちなみにLlamaの場合はなんかちょっと論文見たんですけどちょっと厳密によくわかんなくてなんか参照したよっていうふうに書いてあるんすけど実際ラーニング例とは何かちょっといじってるみたいなので、この辺どうやってるかっていうの多分論文とか図形なモデルによると思うのでちょっとモデルを実際興味ある人は見てもらえばと思います \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "topk = 5\n",
        "for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n",
        "  print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n",
        "  print(documents[index], \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Ow0wZy6ETHIx"
      },
      "outputs": [],
      "source": [
        " #回答に役立つ該当の発言はreference[1871]〜に含まれてます。\n",
        "references = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=False,\n",
        "    # temperature=0.6, # If do_sample=True\n",
        "    # top_p=0.9,  # If do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "z_4dkHGKTPr-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## LLMにおけるInference Time Scalingとは？\n",
            "\n",
            "**Inference Time Scaling**は、**LLM（Large Language Model）の推論時間（Inference Time）をスケールする手法**です。 \n",
            "\n",
            "**簡単に説明すると:**\n",
            "\n",
            "* **スケール:**  LLMの処理能力を拡張し、より多くのデータや複雑なタスクを処理できるようになることを意味します。\n",
            "* **Inference Time Scaling:**  スケールを適用することで、推論時間を効率的に短縮し、より高速で応答できるようになります。\n",
            "\n",
            "**具体的な内容:**\n",
            "\n",
            "* **Pretraining:**  LLMの学習段階で、大量のデータからモデルを学習させる手法。\n",
            "* **スケーリングPretraining:**  Pretraining段階だけでなく、推論段階においてもスケールを適用することで、より高速な推論が可能になります。\n",
            "* **スケーリング:**  推論段階におけるスケール化手法は、モデルの層数や次元数、フィードフォワードの次元、アテンションの数などを調整することで、推論時間を効率的に短縮します。\n",
            "\n",
            "**最近の注目点:**\n",
            "\n",
            "* **スケーリングPretraining:**  Pretraining段階だけでなく、推論段階においてもスケールを適用することで、より\n"
          ]
        }
      ],
      "source": [
        "response = outputs[0][input_ids.shape[-1]:]\n",
        "print(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "answers.append(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn7tih0RTTzr"
      },
      "source": [
        "## 結果 (初期RAG実装)\n",
        "\n",
        "講義内容のドキュメントを追加したにもかかわらず、モデルの回答には依然として以下の問題が見られます：\n",
        "* 「高速に推論する」など、従来の一般的な推論最適化と「Inference Time Scaling」を混同した誤った解釈が継続\n",
        "* 講義内容を参照しているものの、概念の本質を正確に捉えられていない\n",
        "\n",
        "### 問題分析\n",
        "以下の要因が考えられます：\n",
        "1. **ドキュメント品質の問題**: 音声認識による文字起こしの精度不足\n",
        "2. **検索精度の課題**: 単純な文単位の分割では文脈が失われ、関連性の高いドキュメント片を適切に取得できていない可能性\n",
        "\n",
        "### 書き起こしテキストの品質改善\n",
        "\n",
        "日本語の音声認識（speech2text）モデルは精度に課題があることが知られています。以下に「LLMにおけるInference Time Scalingとは？」に関連する講義内容の書き起こしテキストを比較します："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q83QyfAIphk6"
      },
      "source": [
        "### 講義中の該当発言 (LLM講座Day4後半から抜粋)\n",
        "\n",
        "\n",
        "<修正前>\n",
        "---\n",
        "\n",
        "講義に戻ります。ちょっと練習の時間もあるのであと20分ぐらいで駆け足になりますけど、最後最近のスケールトレンドって話で**生のGENIACLM**の話をして終わろうと思いですねちょっとモチベーションから話すと、ちょっと頭で考えてみてほしいとか見れば一瞬で思うとんですけどバナナの色は何ですかって言われたときと、今日の講義聞いた上で、**ゲームソフトの問題は何だと思いますか**って聞かれたとき、多分あの考えることが違うと思うんですね。**羽の色なんですか**っていうと一瞬黄色ですねもしかしたら緑かもしれないけどぐらいですかね物によるかなみたいなおもちゃだったら違うかもみたいな、だんだんあの、考えていくといろいろ出てくるかもしれないすけど、少なくとも**スケール足の問題なんだと思いますか**って聞かれたときに、今日の話からするとスケール則っていうのはこういうものだからどうだろうこの辺が問題かなみたいな考えとやっぱ思考としては違うってことは何となく思うかなと思います。なんか人間的にはこの二つって全然違うしあの、答えるのに必要な考え方っていうのも違うように思えるわけです。**スケールって言ってる7Gのスケール**って言ってるのはこういった形で、あの簡単なものについては簡単に答えてもいいですし、そうじゃなくて、あの考えなきゃいけない問題に対しては、考える時間を、に計算式を使うというふうにしたときに、これいいことがあるのかっていうような話になってます。二つで、ちょっと順番が前後しますけどこれの仕組みは言語モデルでも効果的ですかっていう話と、これをどう実現できるかっていう、こういう二つの話が最近のトレンドとして出てきています。効果的ですかっていうのが、最近**大湾**と呼ばれる論文が論文じゃないか、モデルが**オペル**から出ましたプレビューとして出てますけどこの法案で注目されていますこれあの**論文にROMってかブログ**にあるとイエスって右側が訓練時の計算資源をスケールさせたときに、初めて何かロジックのベンチマークがあるんですけどこれをがどうなったかで何となくスケールしてると右側がテストTimeコンピュートっていうふうに書いてると思うんすけど、**水温時**に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだんコース計算式を増やしていったときに、性能がどう変わるかっていうのでこれもスケールしていってるということがわかると思います。こういった形で、要は考える時間をどうやら推論時に使うと計算資源を推論使うのはいいことがありそうだということがわかります。\n",
        "\n",
        "\n",
        "<修正後>\n",
        "---\n",
        "\n",
        "\n",
        "講義に戻ります。ちょっと演習の時間もあるのであと20分ぐらいで駆け足になりますけど、最後最近のスケールトレンドってことで**「推論時のスケーリング」**についての話をして終わろうと思います。モチベーションから話すと、ちょっと頭で考えてみてもらえれば一瞬でわかると思うとんですけど、「バナナの色は何ですかって言われたとき」と、今日の講義聞いた上で、**「スケール則の問題は何だと思いますか」**って聞かれたとき、多分あの考えることが違うと思うんですね。\n",
        "**「バナナの色なんですか」**っていうと黄色ですね。もしかしたら緑かもしれないけど、物によるかなみたいな、おもちゃだったら違うかもみたいな、だんだんあの、考えていくといろいろ出てくるかもしれないすけど、少なくとも**「スケール則の問題なんだと思いますか」**って聞かれたときに、今日の話からするとスケール則っていうのはこういうものだから「どうだろう」「この辺が問題かな」みたいな考えとはやっぱ思考としては違うってことは何となく思うかなと思います。\n",
        "なんか人間的にはこの二つって全然違うしあの、答えるのに必要な考え方っていうのも違うように思えるわけです。**推論時のスケールって言ってるのは**こういった形で、あの簡単なものについては簡単に答えてもいいですし、そうじゃなくて、深く考えなきゃいけない問題に対しては、考える時間に計算資源を使うというふうにしたときに、これいいことがあるのかっていうような話になってます。\n",
        "これの仕組みは言語モデルでも効果的ですかっていう話と、これをどう実現できるかっていう、こういう二つの話が最近のトレンドとして出てきています。効果的ですかっていうのが、最近**o1**と呼ばれるモデルが**OpenAI**から出ました。プレビューとして出てますけどこのo1で注目されています。これあのo1の**論文ってかブログ**にある図で、左側が訓練時の計算資源をスケールさせたときに、AIMEというロジックのベンチマークがあるんですけど、accuracyがどうなったかというと、何となくスケールしてる。右側がtest-time computeっていうふうに書いてると思うんすけど、**推論時**に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだん計算資源を増やしていったときに、性能がどう変わるかっていうので、これもスケールしていってるということがわかると思います。\n",
        "こういった形で、要は考える時間をどうやら推論時に使うと、つまり計算資源を推論時に使うのはいいことがありそうだということがわかります。\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrp81WzyhYc"
      },
      "source": [
        "---\n",
        "### 文字起こしの誤り\n",
        "\n",
        "上記の比較からわかるように、音声認識による書き起こしには重大な誤りが多数含まれています：\n",
        "* 「スケール則の問題」→「ゲームソフトの問題」\n",
        "* 「o1」→「大湾」\n",
        "といった明らかに文脈に合わない単語変換が発生しています。\n",
        "\n",
        "`LLM2024_day4_raw.txt`の中には、このような誤変換が多数見られます。これらの誤りはRAG性能に直接影響し、モデルの回答精度を低下させる要因となります。\n",
        "\n",
        "したがって、**ドキュメント品質の改善**を行い、RAG性能の向上を図ります。\n",
        "\n",
        "## 講義内容をソースとして活用：改善版RAG実装\n",
        "\n",
        "* **ドキュメント処理**: \n",
        "  - speech2textによる書き起こしテキストを人手で丁寧に修正\n",
        "  - 専門用語（Inference Time Scaling、GPT-o1など）の正確な表記を確保\n",
        "  - 文脈の流れを維持しつつ、文法的に正確な日本語に修正\n",
        "\n",
        "* **検索手法**: \n",
        "  - 引き続き「。」（句点）で区切られた文単位でテキストを分割\n",
        "  - 文単位の検索により、モデルの入力トークン制限内で関連情報を最大化\n",
        "\n",
        "この改善により、モデルが正確な情報に基づいて「Inference Time Scaling」の概念を理解し、適切な回答を生成することが期待されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "WNjIC4RnzkNW"
      },
      "outputs": [],
      "source": [
        "with open(\"./data/LLM2024_day4.txt\", \"r\") as f:\n",
        "# with open(\"/content/lecture-ai-engineering/day3/data/LLM2024_day4.txt\", \"r\") as f:\n",
        "  raw_writedown = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "f53OojeTzkNW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ドキュメントサイズ:  350\n",
            "ドキュメントの例: \n",
            " それからBest of Nとはちょっと違う方法として、N個を生成した後に、それらを集約するという意味では、Day2でやったSelf-Consistencyをこの枠組みの一つとして説明されます\n"
          ]
        }
      ],
      "source": [
        "# ドキュメントを用意する。\n",
        "documents = [text.strip() for text in raw_writedown.split(\"。\")]\n",
        "print(\"ドキュメントサイズ: \", len(documents))\n",
        "print(\"ドキュメントの例: \\n\", documents[310])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "mlduigQ3OfoN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[61.822235107421875, 66.46027374267578, 60.23433303833008, 51.8294563293457, 54.90752410888672, 51.55607604980469, 58.39425277709961, 60.800106048583984, 55.06976318359375, 59.28041458129883, 58.47063064575195, 62.85796356201172, 58.220882415771484, 59.10316848754883, 55.34014892578125, 57.35477066040039, 59.756874084472656, 51.492427825927734, 60.6212272644043, 62.5194091796875, 57.498939514160156, 58.55681228637695, 59.716163635253906, 59.434837341308594, 63.48379898071289, 61.778526306152344, 59.61750030517578, 62.45526123046875, 53.734352111816406, 61.902618408203125, 55.2577018737793, 56.98680877685547, 62.4495735168457, 56.231964111328125, 56.5261116027832, 56.395809173583984, 58.69124984741211, 51.74648666381836, 56.2239875793457, 56.69636154174805, 58.36695861816406, 55.44096374511719, 59.27631759643555, 59.533382415771484, 57.69160842895508, 54.764198303222656, 58.02476501464844, 57.39853286743164, 56.447208404541016, 52.43053436279297, 61.59440231323242, 62.69723129272461, 56.998573303222656, 59.71112823486328, 59.490509033203125, 56.53730773925781, 56.920230865478516, 57.18075180053711, 58.18360900878906, 58.069366455078125, 57.947303771972656, 57.579708099365234, 59.26993942260742, 55.392086029052734, 59.91419982910156, 54.53939437866211, 53.82004928588867, 57.40055847167969, 62.655494689941406, 56.26073455810547, 50.57143020629883, 60.72755432128906, 63.173973083496094, 54.54304122924805, 60.404510498046875, 62.06141662597656, 64.0365982055664, 58.501808166503906, 56.74510192871094, 60.12650680541992, 60.04476547241211, 65.01075744628906, 66.20555877685547, 60.685157775878906, 60.019248962402344, 56.81828308105469, 61.33220291137695, 59.938262939453125, 61.120384216308594, 55.29457473754883, 62.148345947265625, 63.139068603515625, 59.7241096496582, 57.20750045776367, 56.25497817993164, 60.36245346069336, 57.4828987121582, 57.26449966430664, 58.51197814941406, 62.256874084472656, 58.3666877746582, 59.343284606933594, 53.60014343261719, 59.00265121459961, 52.59321212768555, 55.59575653076172, 61.40422821044922, 56.936336517333984, 54.40184020996094, 61.938941955566406, 55.81211471557617, 52.55049514770508, 61.49480438232422, 59.53042984008789, 59.60274887084961, 59.10978317260742, 57.18939208984375, 59.20886993408203, 61.62041473388672, 53.66431427001953, 54.28034591674805, 59.06022644042969, 61.03776550292969, 57.18226623535156, 58.59380340576172, 59.66725158691406, 61.4532356262207, 50.23600387573242, 52.51435089111328, 60.79493713378906, 59.194435119628906, 50.23601150512695, 51.401771545410156, 58.31843566894531, 58.71158981323242, 55.09911346435547, 55.518714904785156, 53.50701904296875, 58.31631851196289, 53.980838775634766, 57.042205810546875, 60.90351486206055, 60.342323303222656, 52.573001861572266, 55.289154052734375, 52.88311767578125, 61.406166076660156, 54.58738708496094, 60.88069152832031, 63.513362884521484, 60.71745300292969, 57.03582763671875, 58.82558822631836, 55.42204284667969, 55.17709732055664, 60.46223449707031, 63.97543716430664, 48.75161361694336, 52.58493423461914, 54.21562194824219, 55.074440002441406, 57.766510009765625, 58.6453971862793, 57.04252243041992, 52.210540771484375, 60.97018814086914, 52.46037292480469, 56.71750259399414, 59.376930236816406, 56.86631774902344, 55.2564697265625, 58.702003479003906, 58.0469856262207, 53.55220031738281, 52.725685119628906, 58.84385681152344, 58.90950012207031, 55.08743667602539, 56.79095458984375, 54.55354309082031, 58.081077575683594, 53.225379943847656, 56.831390380859375, 57.25721740722656, 57.208038330078125, 58.24198913574219, 56.68033981323242, 62.582862854003906, 57.183956146240234, 60.021697998046875, 60.72306823730469, 60.72357940673828, 60.264732360839844, 58.830894470214844, 56.8951530456543, 59.55482864379883, 64.62336730957031, 64.72208404541016, 61.05486297607422, 55.56547546386719, 62.32646179199219, 56.689476013183594, 57.02949905395508, 64.38357543945312, 54.64978790283203, 62.13221740722656, 59.65718078613281, 57.02830505371094, 57.63664245605469, 60.35309982299805, 59.749114990234375, 55.82729721069336, 60.05771255493164, 51.46577835083008, 58.2193489074707, 58.96762466430664, 53.79920196533203, 57.68940353393555, 58.01826858520508, 56.064476013183594, 58.87654495239258, 55.524658203125, 58.9599494934082, 57.673973083496094, 58.3535270690918, 53.34004211425781, 54.13231658935547, 57.74562454223633, 56.989723205566406, 58.210289001464844, 56.83745574951172, 56.66750717163086, 56.96505355834961, 63.239036560058594, 52.39265060424805, 55.765140533447266, 59.11424255371094, 57.75636291503906, 52.150760650634766, 60.90528869628906, 58.45421600341797, 60.203975677490234, 59.19042205810547, 55.515472412109375, 63.6585693359375, 61.68105697631836, 65.28672790527344, 56.931884765625, 60.18281555175781, 53.2462043762207, 62.611412048339844, 59.17506408691406, 63.688087463378906, 60.90778732299805, 57.66341781616211, 60.02001953125, 57.24964141845703, 57.722198486328125, 55.75218200683594, 58.569217681884766, 55.44620895385742, 57.76676940917969, 59.783966064453125, 59.3012580871582, 53.77507781982422, 59.055572509765625, 59.3261604309082, 65.18833923339844, 59.662574768066406, 61.347801208496094, 53.50114059448242, 64.76490783691406, 55.30813217163086, 48.962371826171875, 56.122982025146484, 53.85411834716797, 63.79270553588867, 58.533321380615234, 58.32482147216797, 53.33759689331055, 64.68008422851562, 66.5853042602539, 63.129554748535156, 62.019569396972656, 56.95514678955078, 59.11124801635742, 56.0828971862793, 65.24553680419922, 55.77674102783203, 58.77827835083008, 54.82826232910156, 54.26567840576172, 51.08429718017578, 61.75537109375, 53.2790412902832, 59.685218811035156, 59.45201110839844, 56.83027648925781, 59.547889709472656, 54.36494445800781, 49.5715217590332, 58.46632766723633, 52.60207748413086, 56.50498962402344, 55.15361022949219, 63.6822509765625, 56.0748176574707, 58.6657829284668, 58.009483337402344, 55.93758773803711, 55.941123962402344, 56.818031311035156, 56.77470016479492, 56.58439254760742, 58.4090576171875, 53.4881706237793, 56.35490417480469, 58.16584396362305, 54.31282424926758, 57.03287124633789, 55.78336715698242, 58.80004119873047, 59.08736038208008, 59.225250244140625, 55.96905517578125, 58.378517150878906, 53.33131408691406, 53.96794128417969, 59.136260986328125, 54.18995666503906, 56.7227783203125, 54.170143127441406, 53.75917434692383, 56.58589172363281, 54.53624725341797, 57.88365173339844, 54.972984313964844, 57.510650634765625, 56.35135269165039, 55.35146713256836, 57.474327087402344, 57.3939323425293, 54.42156219482422, 54.127716064453125, 53.8560676574707, 64.7896957397461, 58.541255950927734, 67.37751770019531, 55.7373161315918, 50.88081359863281]]\n"
          ]
        }
      ],
      "source": [
        "# Retrievalの実行\n",
        "question = \"LLMにおけるInference Time Scalingとは？\"\n",
        "\n",
        "query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n",
        "document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "# 各ドキュメントの類似度スコア\n",
        "scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "print(scores.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "FNsGUsnlOoMm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "取得したドキュメント1: (Score: 67.37751770019531)\n",
            "最後に補足して僕のパート終わろうと思いますけど、同じ計算資源のときにパラメータ増やすのよりも推論資源を増やすのが有効なのかっていうのが問いとしてあると思いますけど、o1の場合だと、訓練時のスケールは同じままって推論時のスケールを増やしたら、より賢くなりましたって話でしたけど、どっちにするのがいいのかっていう意味で言うと、GoogleDeepMindが8月に論文としてまして、Scaling LLM Test-Time Comupte Optimally can be more Effective than Scaling More Paremetersっていうことで、良いらしいというふうに言われてます \n",
            "\n",
            "\n",
            "取得したドキュメント2: (Score: 66.5853042602539)\n",
            "右側がtest-time computeっていうふうに書いてると思うんすけど、推論時に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだん計算資源を増やしていったときに、性能がどう変わるかっていうので、これもスケールしていってるということがわかると思います \n",
            "\n",
            "\n",
            "取得したドキュメント3: (Score: 66.46027374267578)\n",
            "あのスケールするっていうところではタイトルの通りなんですけど、ちょっとこれスケーリングPretraining回ってなってるんですけれども、ちょっと最近はですね、このPretrainingだけではなくて、推論をスケールさせるというような話も出てきてましてせっかくなのでその最近の話題ということです推論時のスケーリングことで、ちょっとタイトル詐欺が入ってるんですけどPretrainingだけじゃない、スケーリングも扱うということで、ちょっと若干あのタイトル詐欺なんですけども、あの最近の話題ということで推論時のスケジュールについても話していきたいなと思っています \n",
            "\n",
            "\n",
            "取得したドキュメント4: (Score: 66.20555877685547)\n",
            "Trasnformerの場合はスケール則が、パラメータ数が横軸になってますけどこういうふうになると、LSTMの場合には1層2層4層みたいにそれぞれスケール則を解くとこんなふうになりますよということで、Trasnformer以外のスケール則っていうのもあの検証をされている \n",
            "\n",
            "\n",
            "取得したドキュメント5: (Score: 65.28672790527344)\n",
            "気にしながらっていうのの実例を出した方がわかりやすいと思うので、実際にこれ開発者じゃないので、あの結果を見て推論してるだけなんで嘘ついてるかもしれないですけど例えばLlama3の論文を持ってくると8Billon,70Billon,405Billonで層の数、モデルDimension、埋め込みの数次元ですね、フィードフォワードの次元、アテンションの数っていうのを、こういうふうにしたよっていうふうに言われてます \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "topk = 5\n",
        "for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n",
        "  print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n",
        "  print(documents[index], \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "MoOCvFW4ltcA"
      },
      "outputs": [],
      "source": [
        " #回答に役立つ該当の発言はreference[1871]〜に含まれてます。\n",
        "references = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=False,\n",
        "    # temperature=0.6, # If do_sample=True\n",
        "    # top_p=0.9,  # If do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "2FbzMLfTtWxx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## LLMにおけるInference Time Scalingとは？\n",
            "\n",
            "**Inference Time Scaling**とは、**推論時に計算資源を増やす**ことを指します。 \n",
            "\n",
            "**具体的には:**\n",
            "\n",
            "* **Pretraining**: モデルを訓練する際に使用する計算資源\n",
            "* **Inference**: モデルを実際に利用する際に使用する計算資源\n",
            "\n",
            "**Inference Time Scalingのメリット:**\n",
            "\n",
            "* **スケールする**:  モデルの規模を大きくするのではなく、推論時の計算資源を増やすことで、性能を向上させることが可能になります。\n",
            "* **パラメータ数の増加よりも効果的**: GoogleDeepMindの研究では、推論時の計算資源を増やす方が、より効果的であることが示唆されています。\n",
            "\n",
            "\n",
            "**Inference Time Scalingの具体的な方法:**\n",
            "\n",
            "* **Pretraining**: モデルの学習時に使用する計算資源\n",
            "* **Inference**: モデルの利用時に使用する計算資源\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = outputs[0][input_ids.shape[-1]:]\n",
        "print(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "answers.append(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLe0IJPeH97d"
      },
      "source": [
        "## 結果 (修正テキストによるRAG)\n",
        "\n",
        "書き起こしテキストの品質改善により、モデルの回答に部分的な向上が見られました：\n",
        "\n",
        "### 改善点\n",
        "* 「推論時（Inference）に計算資源をスケーリングすることで、モデルがより賢くなり、性能が向上すること」という概念を正確に捉えるようになった\n",
        "\n",
        "### 問題点\n",
        "* 「Inference Time Scalingは、TransformerやLSTMなどのモデルにおいて、パラメータ数を増やすのではなく、推論時計算資源をスケーリングすることで、性能が向上すること...」という記述は講義内容と矛盾している\n",
        "\n",
        "### 問題分析\n",
        "\n",
        "モデルが誤った回答を生成する主要因として、**文脈の欠如**が考えられます：\n",
        "* 「。」で区切られた短い文単位での検索では、各文の発言背景や関連性が失われる\n",
        "* 単独の文から情報を抽出するため、講師の全体的な主張や議論の流れを把握できない\n",
        "* 結果として、正しい個別の文でも、その解釈に必要な背景情報が欠如し、誤った文脈で理解される\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 3. 文脈を考慮したチャンク化の導入\n",
        "\n",
        "検索結果の品質向上のため、以下の改善を実施します：\n",
        "\n",
        "* **前後文脈を含むチャンク化**:\n",
        "  - 検索でマッチした文だけでなく、その前後の複数文も含めてチャンクとして取得\n",
        "  - 具体的には、マッチした文を中心に前2文、後2文を含む計5文程度のチャンクを構成\n",
        "  - この「文脈ウィンドウ」により、発言の背景情報や議論の流れが保持される\n",
        "\n",
        "* **期待される効果**:\n",
        "  - 講師の主張とその根拠の関係性を正確に把握できる\n",
        "  - 概念の定義とその適用範囲を正しく理解できる\n",
        "\n",
        "この改善により、モデルが講義内容の本質をより正確に理解し、一貫性のある事実に基づいた回答を生成することが期待されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "94uovDFrVOTJ"
      },
      "outputs": [],
      "source": [
        "# 前後それぞれ2つずつの文章を一つのドキュメントに追加する。（要は5つの文章集合になる)\n",
        "references = \"\\n\".join([\"* \" + \"。\".join(documents[max(0, i-2): min(i+2, len(documents))]).strip() for i in scores.argsort()[0][::-1][:topk]])\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=False,\n",
        "    # temperature=0.6, # If do_sample=True\n",
        "    # top_p=0.9,  # If do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "SAzYsxVWVdMS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## LLMにおけるInference Time Scalingとは？\n",
            "\n",
            "**Inference Time Scaling**とは、**推論時の計算資源を増やすことで、モデルの性能を向上させる**ことを指します。 \n",
            "\n",
            "具体的には、**モデルの学習段階（Pretraining）で計算資源を増やす**だけでなく、**推論段階（Inference）で計算資源を増やす**という方法も注目されています。\n",
            "\n",
            "\n",
            "**Inference Time Scalingの目的**\n",
            "\n",
            "* **計算資源の効率性向上:**  推論時に必要な計算資源を効率的に使用することで、コスト削減と実行時間の短縮を実現します。\n",
            "* **モデル性能の向上:**  推論時の計算資源を増やすことで、モデルの精度や処理能力を向上させることが期待されます。\n",
            "\n",
            "\n",
            "\n",
            "**Inference Time Scalingの現状**\n",
            "\n",
            "* **Pretraining段階でのスケール:**  従来のモデルでは、Pretraining段階で計算資源を増やすことが主流でした。\n",
            "* **推論段階でのスケール:**  近年、推論段階でのスケールも注目され、より効率的なモデル開発が進んでいます。\n",
            "* **スケール則の研究:**  スケール則を研究することで、最適なスケール方法を特定し、より効率的なモデル開発を実現します。\n",
            "\n",
            "\n",
            "\n",
            "**Inference Time\n"
          ]
        }
      ],
      "source": [
        "response = outputs[0][input_ids.shape[-1]:]\n",
        "print(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "answers.append(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD3R54G1WX8B"
      },
      "source": [
        "## 結果 (文脈付きチャンク化によるRAG)\n",
        "\n",
        "文脈を含むチャンク化により、モデルの回答の方向性に明確な改善が見られました：\n",
        "\n",
        "### 改善点\n",
        "* 「推論時の計算をスケールさせる」という概念を据えて回答\n",
        "* Inference Time Scalingの基本原理についての理解が向上\n",
        "\n",
        "### 残存する問題点\n",
        "* 質問と関連性の低い情報（ノイズ）が混入する\n",
        "\n",
        "### 問題分析\n",
        "\n",
        "文脈付きチャンク化によるアプローチで新たに発生した課題：\n",
        "\n",
        "1. **情報過多の問題**:\n",
        "   * ドキュメント量の増加により、モデルに提供される情報総量が大幅に増加\n",
        "   * 関連情報と非関連情報が混在し、ノイズと重要情報の区別が困難に\n",
        "\n",
        "2. **情報選択の複雑化**:\n",
        "   * モデルは単に回答を生成するだけでなく、提供された多様な情報源から関連性の高い情報を選別する作業も担うことになった\n",
        "   * この二重タスクにより回答生成の難易度が上昇\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 4. Rerankによる情報品質の向上\n",
        "\n",
        "検索精度をさらに向上させるため、二段階の検索プロセスを導入します：\n",
        "\n",
        "* **Rerank手法の導入**:\n",
        "  - 第一段階: 従来通り基本的な検索アルゴリズムでtop-k個のドキュメントチャンクを取得\n",
        "  - 第二段階: 取得したチャンクに対してLLMを活用した高度な関連性評価を実施\n",
        "  - LLMに「このドキュメントは質問『LLMにおけるInference Time Scalingとは？』に対して本当に関連性が高いか」を判断させる\n",
        "  - 関連性スコアに基づいてランク付けし、真に関連性の高いチャンクのみを選出\n",
        "\n",
        "* **期待される効果**:\n",
        "  - 質の高い情報に焦点を絞ることで、ノイズとなる情報を大幅に削減\n",
        "  - 文脈を保ちながらも、関連性の高い情報のみをモデルに提供\n",
        "  - モデルのタスクを「多量の情報から選別して回答」から「厳選された情報に基づいて回答」へと単純化\n",
        "\n",
        "この高度な情報フィルタリングにより、Inference Time Scalingに関する正確で一貫性のある回答生成が期待されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "2HfzJ5EpXGtj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "対象となるドキュメント:\n",
            " これでほぼちょうどですけど、最後に少しあの、前半では全体の訓練時のスケーリングをする話を基本的にしましたけど、最近ではこの推論時の計算量っていうのも注目するような研究が増えてきています。\n",
            "代表的なGPT-o1とかですごく注目されてるかなと思いますし、今までやった方法、学んだ方法も結構出てきたと思いますけど、Promptingを工夫するとか、Decodingを工夫するとかいうので、それにも発展的な方法がいろいろ出てきていますし、Meta Generationっていうような枠組みで、DecodingだけじゃなくてそのDecodeした結果を最後どう使うかみたいな含めて、Meta Generationというふうに呼んでますけど、Paralell SearchとかStep Level SearchとかRefinementと言われるような枠組みの研究も出てきていますというような話をしました。\n",
            "最後に補足して僕のパート終わろうと思いますけど、同じ計算資源のときにパラメータ増やすのよりも推論資源を増やすのが有効なのかっていうのが問いとしてあると思いますけど、o1の場合だと、訓練時のスケールは同じままって推論時のスケールを増やしたら、より賢くなりましたって話でしたけど、どっちにするのがいいのかっていう意味で言うと、GoogleDeepMindが8月に論文としてまして、Scaling LLM Test-Time Comupte Optimally can be more Effective than Scaling More Paremetersっていうことで、良いらしいというふうに言われてます。\n",
            "厳密に言うとこれなんかタスクによって違うということなので、良いとまで言っていいのかちょっと若干誇大広告な気が個人的にはしてますけど、そういったことを検証するような研究も出てきていますので興味ある人は見てもらえばと思います\n",
            "\n",
            "関連しているかどうか:  yes \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "対象となるドキュメント:\n",
            " プレビューとして出てますけどこのo1で注目されています。\n",
            "これあのo1の論文ってかブログにある図で、左側が訓練時の計算資源をスケールさせたときに、AIMEというロジックのベンチマークがあるんですけど、accuracyがどうなったかというと、何となくスケールしてる。\n",
            "右側がtest-time computeっていうふうに書いてると思うんすけど、推論時に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだん計算資源を増やしていったときに、性能がどう変わるかっていうので、これもスケールしていってるということがわかると思います。\n",
            "こういった形で、要は考える時間をどうやら推論時に使うと、つまり計算資源を推論時に使うのはいいことがありそうだということがわかります\n",
            "\n",
            "関連しているかどうか:  yes \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "対象となるドキュメント:\n",
            " 早速内容ですけど、目的はタイトルの通りですけど言語モデルスケール則について学ぶってことで、大規模言語モデルっていうふうに呼ばれてますけど、ちょっとスケール則の話とか初回も少ししましたけど、これだけ大きくなっている一つの理由になってますのでそのスケール則ってどういうものなのかとかそれがなぜ重要なのかっていうところ、説明できるようなってもらうというところと、スケール則ってどうやって求めるんでしたっけというところを説明実装できるようになるところについて中心的に話していければと思ってます。\n",
            "あのスケールするっていうところではタイトルの通りなんですけど、ちょっとこれスケーリングPretraining回ってなってるんですけれども、ちょっと最近はですね、このPretrainingだけではなくて、推論をスケールさせるというような話も出てきてましてせっかくなのでその最近の話題ということです推論時のスケーリングことで、ちょっとタイトル詐欺が入ってるんですけどPretrainingだけじゃない、スケーリングも扱うということで、ちょっと若干あのタイトル詐欺なんですけども、あの最近の話題ということで推論時のスケジュールについても話していきたいなと思っています。\n",
            "演習では2つ目のポイントに近いですけどスケール則を実際に求めるというところでそのコードを実装できるようになってもらうということを目的としています\n",
            "\n",
            "関連しているかどうか:  yes \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "対象となるドキュメント:\n",
            " それをスケールアップさせたのが先ほどのOpenAIの研究だというふうに説明できるかなと思います。\n",
            "それから元のOpenAIの論文に戻りますと、今言ったようなLSTMの比較みたいなLSTMにおけるスケール則みたいなことも、この論文でも検証されていまして、左側がモデル構造が違うんですね。\n",
            "Trasnformerの場合はスケール則が、パラメータ数が横軸になってますけどこういうふうになると、LSTMの場合には1層2層4層みたいにそれぞれスケール則を解くとこんなふうになりますよということで、Trasnformer以外のスケール則っていうのもあの検証をされている。\n",
            "深さについても検証してまして、これも元のモデルが何だったかちょっと忘れちゃったけど、確かLSTMだったような気がしますけど、層を変えたときにどういうふうな変化するかっていうのをこういった形でプロットするようなことがされてます\n",
            "\n",
            "関連しているかどうか:  no \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "対象となるドキュメント:\n",
            " こういったものを変えて実験してみたっていうのが最初の最初じゃないOpenAIのScaling Lawで話されていました。\n",
            "基本的にはこの辺見るとなんかあんまり性能に影響ないっていうふうにこの論文では言ってますけど、この辺を気にしながらモデルスケールすることが多いです。\n",
            "気にしながらっていうのの実例を出した方がわかりやすいと思うので、実際にこれ開発者じゃないので、あの結果を見て推論してるだけなんで嘘ついてるかもしれないですけど例えばLlama3の論文を持ってくると8Billon,70Billon,405Billonで層の数、モデルDimension、埋め込みの数次元ですね、フィードフォワードの次元、アテンションの数っていうのを、こういうふうにしたよっていうふうに言われてます。\n",
            "これさっき言ったアスペクト比、縦横比がこのモデルdimentionをLayerで割ったものなんで、これそれぞれ見ると128,102.4,130ってことでこれ大体100から130ぐらい、なんかおおむね同じような値になっていることがわかると思います\n",
            "\n",
            "関連しているかどうか:  yes \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        " #回答に役立つ該当の発言はreference[1871]〜に含まれてます。\n",
        "references = []\n",
        "for ref in [\"。\".join(documents[max(0, i-2): min(i+2, len(documents))]).strip() for i in scores.argsort()[0][::-1][:topk]]:\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": f\"与えられた[参考資料]が[質問]に直接関連しているかを、'yes''no'で答えること。[参考資料]\\n{ref}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"},\n",
        "  ]\n",
        "  input_ids = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\"\n",
        "  ).to(model.device)\n",
        "\n",
        "  terminators = [\n",
        "      tokenizer.eos_token_id,\n",
        "      tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  ]\n",
        "\n",
        "  outputs = model.generate(\n",
        "      input_ids,\n",
        "      # max_new_tokens=128,\n",
        "      eos_token_id=terminators,\n",
        "      do_sample=False,\n",
        "      # temperature=0.6, # If do_sample=True\n",
        "      # top_p=0.9,  # If do_sample=True\n",
        "  )\n",
        "\n",
        "  response = outputs[0][input_ids.shape[-1]:]\n",
        "  response = tokenizer.decode(response, skip_special_tokens=True)\n",
        "  print(\"\\n\\n対象となるドキュメント:\\n\", ref.replace(\"。\", \"。\\n\"))\n",
        "  print(\"\\n関連しているかどうか: \", response)\n",
        "\n",
        "  if \"yes\" in response.lower():\n",
        "    references.append(ref)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "cLietDaD5I3h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        }
      ],
      "source": [
        "print(len(references))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs74h4ADXj99"
      },
      "source": [
        "上記より、上位4件のみが関連しているとわかったので、これらだけをモデルに渡すこととする。（生成内容が確立的なので、4件でない可能性もあります）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "Fu9wBykZXxja"
      },
      "outputs": [],
      "source": [
        " #回答に役立つ該当の発言はreference[1871]〜に含まれてます。\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": f\"与えられる資料を参考にして回答すること。[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=False,\n",
        "    # temperature=0.6, # If do_sample=True\n",
        "    # top_p=0.9,  # If do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "z5kHntvSXxjb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## LLMにおけるInference Time Scalingとは？\n",
            "\n",
            "Inference Time Scalingとは、**推論時に計算資源を増やす**ことを指します。 \n",
            "\n",
            "**具体的には:**\n",
            "\n",
            "* **Pretraining**段階でモデルを学習させる際に、計算資源を増やす\n",
            "* **Inference**段階で、モデルを実際に利用する際に、計算資源を増やす\n",
            "\n",
            "**目的:**\n",
            "\n",
            "* **性能向上:** 推論速度を高速化し、より迅速な回答を実現\n",
            "* **スケール性の拡張:**  より複雑なタスクに対応できるようになる\n",
            "\n",
            "\n",
            "**最近の研究:**\n",
            "\n",
            "* **Pretrainingのみでは限界:**  モデルの規模が大きくなると、Inference Time Scalingが重要になってくる\n",
            "* **Inference Time Scalingの効果:**  Pretraining段階でのスケールと比較して、Inference Time Scalingの方が効果的であることが示唆されている\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = outputs[0][input_ids.shape[-1]:]\n",
        "print(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "answers.append(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elqD2gJt5RCo"
      },
      "source": [
        "## 結果 (Rerank導入後)\n",
        "\n",
        "Rerankの導入により、回答品質に改善が見られました：\n",
        "\n",
        "### 達成された成果\n",
        "* Inference Time Scalingに関する正確な情報を含んだ回答の生成\n",
        "* 無関係な情報やノイズの排除\n",
        "* 講義内容を反映した説明の実現 🎉\n",
        "\n",
        "この結果から、RAGパイプラインにおける情報の質と関連性の重要性であり、検索で取得した情報を単に増やすだけでなく、その情報の関連性を精査する方法を学ぶことができました。\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 5. さらなる改善案: 意味的チャンク化\n",
        "\n",
        "文単位での分割と前後文脈の追加という現在のアプローチをさらに発展させる手法として、**意味的なチャンク化**が考えられます：\n",
        "\n",
        "* **意味的チャンク（段落）単位での分割**:\n",
        "  - 単純な文の区切りではなく、意味的なまとまり（トピック、議論、例示など）に基づいてテキストを分割\n",
        "  - 人間の主観に基づく意味的な段落分けを活用\n",
        "  - 各チャンクが「一つの完結した考え」を表現するようにする\n",
        "\n",
        "* **期待される効果**:\n",
        "  - より自然な文脈理解が可能に（人間の思考や会話の流れに近い）\n",
        "  - トピックの開始から結論までの流れを維持できる\n",
        "  - 概念間の関係性や比較が同一チャンク内に含まれ、より深い理解につなげる\n",
        "\n",
        "* **検証方法**:\n",
        "  - 人間が主観的に意味でグループ化したチャンクセットを用意\n",
        "  - 同じRerank手法を適用し、文単位チャンクとの性能差を比較\n",
        "  - 回答の正確性、一貫性、網羅性を評価指標として使用\n",
        "\n",
        "この意味的チャンク化手法は、特に講義のような構造化された発話においては、より自然で効果的な情報検索と理解を可能にすると予想されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU_ttvcNKayo"
      },
      "source": [
        "**注意事項**\n",
        "\n",
        "**ここから先のセルを実行した場合、GPUメモリ不足になる可能性が高いです。**\n",
        "\n",
        "\n",
        "このノートブックでは、GPUはT4を使用しています。\n",
        "Colab Pro等を契約し、L4などのよりGPUメモリの大きいものを使用するか、モデルやその設定等を変更するなどの工夫が必要になります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "4DNOyPNXAtl3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ドキュメントサイズ:  45\n",
            "ドキュメントの例: \n",
            " 具体的な求め方についても話します。 さっきからチラチラ言ってた通りなんすけど基本的にこれどう図るかっていうと、基本的にはいくつかの条件で実験してフィッティングするって言ってんのは、すごい単純に言ってしまうとそうなります。左側GPT4の論文から取ってきた図で説明したもんですけど、グレーのやつを例えば実験してみて、これぐらいのロスになるんだなっていうので、フィッティングするとこういうカーブになります。 ちなみにこれ、なんでこれ直線にならないんだっていうのをすぐ説明しなかったですがこれ縦軸が実は普通のロスと違ってBits-per-wordっていうのになってて、多分2乗スケールのロスになってるからだと思います。 右側も同じですね。この各点について何かいろんな設定で実験してやって、それを結果を見るということをしてますけどよくよく考えるとスケールさせるときにモデルサイズどうすればいいんでしたっけとか、何をどういじるとモデルサイズが大きくなるんでしたっけ、どういうふうに言えばいいんでしたっけとかですね。 あのモデルサイズ変えたときにハイパーパラメータってどうすんでしたっけそういった細かい問題が出てくる。最初の方ですけどモデルサイズどう変化させるかっていうので、前回やった、こういう図があると思いますけどモデルサイズ変えようと思ったら別にパラメータ、層の数を増やしても、いいわけですし、この埋め込みの次元各tokenの次元を増やしてもいいわけですし、各随所に出てくるこのフィードフォワードネットワークっていうのの中間層の次元を上げてもいいですしヘッドを増やしてもそういうのあのパラメータ自体は上がるということで、これどれをどのぐらいやるんですかっていうのが細かく考えると重要になってきます。 この辺は元の論文でも一応議論されてまして、これ三つほど出してるんすけど例えば真ん中のがアスペクト比っていう、モデルのエンベディングのサイズですね。dモデルっていうものを層数で割ったもの、アスペクト比という縦横比みたいなもので幅と深さの比率をアスペクト比っていうふうにこの論文では呼んでいますけど。こういったものを変えて実験してみたっていうのが最初の最初じゃないOpenAIのScaling Lawで話されていました。基本的にはこの辺見るとなんかあんまり性能に影響ないっていうふうにこの論文では言ってますけど、この辺を気にしながらモデルスケールすることが多いです。 気にしながらっていうのの実例を出した方がわかりやすいと思うので、実際にこれ開発者じゃないので、あの結果を見て推論してるだけなんで嘘ついてるかもしれないですけど例えばLlama3の論文を持ってくると8Billon,70Billon,405Billonで層の数、モデルDimension、埋め込みの数次元ですね、フィードフォワードの次元、アテンションの数っていうのを、こういうふうにしたよっていうふうに言われてます。 これさっき言ったアスペクト比、縦横比がこのモデルdimentionをLayerで割ったものなんで、これそれぞれ見ると128,102.4,130ってことでこれ大体100から130ぐらい、なんかおおむね同じような値になっていることがわかると思います。 それからモデルとフィードフォワードの次元数ですね、モデル次元数に対しフィードフォワードの次元数は3.5倍になっているということがわかります。これ約3.5かな。ちょっと自信ないですちょっとちゃんと計算したとかいった計算したら、ちょっと違ってたら教えてほしいんすけど大体3.5倍ぐらいあったとアテンションのヘッドはこのFFNの次元数と同様にスケールしたモデルの次元と同様にスケールしているということがわかる。 こういった感じで幅とかを大体同じような係数で、なるべく伸ばしてくと、ただこれ、指定したパラメータ数にしようと思ったときに、当然どっかは完全には固定できないので、若干変わりますけど大体同じような比率でスケールさせているというようなことがわかると思います。\n"
          ]
        }
      ],
      "source": [
        "# 本来は段落をそのままdocumentsに入れずに一定のサイズに分割した方が良いでしょうが、簡単のために段落をそのまま入れてしまいます。\n",
        "documents = [text.replace(\"\\n\", \" \").strip() for text in raw_writedown.split(\"\\n\\n\")]\n",
        "print(\"ドキュメントサイズ: \", len(documents))\n",
        "print(\"ドキュメントの例: \\n\", documents[30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "FF6wc10RAxuc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[64.80940246582031, 61.82463073730469, 62.279415130615234, 62.75594711303711, 63.07190704345703, 61.71656799316406, 59.56270217895508, 59.89581298828125, 61.3881721496582, 62.31979751586914, 61.01363754272461, 65.68667602539062, 65.840576171875, 60.71131134033203, 62.971527099609375, 62.90633010864258, 61.8266716003418, 63.1424560546875, 60.52707290649414, 59.37120819091797, 59.915531158447266, 63.584476470947266, 59.79716873168945, 58.89724349975586, 62.60258102416992, 63.643890380859375, 61.1680793762207, 61.81727600097656, 60.26072692871094, 61.51399612426758, 64.97259521484375, 63.2974967956543, 63.33731460571289, 63.41263198852539, 65.70318603515625, 64.9515380859375, 58.451175689697266, 60.1381721496582, 60.07516860961914, 58.61343765258789, 57.92119598388672, 59.45044708251953, 58.8979377746582, 63.05853271484375, 68.84087371826172]]\n"
          ]
        }
      ],
      "source": [
        "question = \"LLMにおけるInference Time Scalingとは？\"\n",
        "\n",
        "query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n",
        "document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "print(scores.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "H-FKkAcTA-Sx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "取得したドキュメント1: (Score: 68.84087371826172)\n",
            "最後に補足して僕のパート終わろうと思いますけど、同じ計算資源のときにパラメータ増やすのよりも推論資源を増やすのが有効なのかっていうのが問いとしてあると思いますけど、o1の場合だと、訓練時のスケールは同じままって推論時のスケールを増やしたら、より賢くなりましたって話でしたけど、どっちにするのがいいのかっていう意味で言うと、GoogleDeepMindが8月に論文としてまして、Scaling LLM Test-Time Comupte Optimally can be more Effective than Scaling More Paremetersっていうことで、良いらしいというふうに言われてます。厳密に言うとこれなんかタスクによって違うということなので、良いとまで言っていいのかちょっと若干誇大広告な気が個人的にはしてますけど、そういったことを検証するような研究も出てきていますので興味ある人は見てもらえばと思います。 \n",
            "\n",
            "\n",
            "取得したドキュメント2: (Score: 65.840576171875)\n",
            "それから元のOpenAIの論文に戻りますと、今言ったようなLSTMの比較みたいなLSTMにおけるスケール則みたいなことも、この論文でも検証されていまして、左側がモデル構造が違うんですね。 Trasnformerの場合はスケール則が、パラメータ数が横軸になってますけどこういうふうになると、LSTMの場合には1層2層4層みたいにそれぞれスケール則を解くとこんなふうになりますよということで、Trasnformer以外のスケール則っていうのもあの検証をされている。深さについても検証してまして、これも元のモデルが何だったかちょっと忘れちゃったけど、確かLSTMだったような気がしますけど、層を変えたときにどういうふうな変化するかっていうのをこういった形でプロットするようなことがされてます。 ポイントはTrasnformer以外でも別にあのスケールするっていうのは成立概念だということです。なんでこんなTrasnformerだけ注目されてるのかってのは後で話します。 \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 簡単のためにtop2でやります。結果を見てもらえれば問題なく関連する項目のみ取得できているのが分かるかと思います。\n",
        "topk = 2\n",
        "for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n",
        "  print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n",
        "  print(documents[index], \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "KtC-wsj4BGwn"
      },
      "outputs": [],
      "source": [
        "reference = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": f\"[参考資料]\\n{reference}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    # max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=False,\n",
        "    # temperature=0.6, # If do_sample=True\n",
        "    # top_p=0.9,  # If do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "c27VI95SCzV5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**LLMにおけるInference Time Scalingとは、推論時の計算資源を増やすことで、モデルの\n"
          ]
        }
      ],
      "source": [
        "response = outputs[0][input_ids.shape[-1]:]\n",
        "print(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "answers.append(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "OJVL3u6lCc8k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n",
            "Answer 1: ## LLMにおけるInference Time Scalingとは？\n",
            "\n",
            "**Inference Time Scaling**とは、Large Language Model (LLM) の推論時間（Inference Time）を効率的に調整する方法です。 \n",
            "\n",
            "**従来の推論時間:**\n",
            "\n",
            "* LLM\n",
            "Answer 2: ## LLMにおけるInference Time Scalingとは？\n",
            "\n",
            "**Inference Time Scaling**は、**LLM（Large Language Model）の推論時間（Inference Time）をスケールする手法**です。 \n",
            "\n",
            "**簡単に説明すると:**\n",
            "\n",
            "* **ス\n",
            "Answer 3: ## LLMにおけるInference Time Scalingとは？\n",
            "\n",
            "**Inference Time Scaling**とは、**推論時に計算資源を増やす**ことを指します。 \n",
            "\n",
            "**具体的には:**\n",
            "\n",
            "* **Pretraining**: モデルを訓練する際に使用する計算資源\n",
            "* **Inf\n",
            "Answer 4: ## LLMにおけるInference Time Scalingとは？\n",
            "\n",
            "**Inference Time Scaling**とは、**推論時の計算資源を増やすことで、モデルの性能を向上させる**ことを指します。 \n",
            "\n",
            "具体的には、**モデルの学習段階（Pretraining）で計算資源を増やす**だ\n",
            "Answer 5: ## LLMにおけるInference Time Scalingとは？\n",
            "\n",
            "Inference Time Scalingとは、**推論時に計算資源を増やす**ことを指します。 \n",
            "\n",
            "**具体的には:**\n",
            "\n",
            "* **Pretraining**段階でモデルを学習させる際に、計算資源を増やす\n",
            "* **Infe\n",
            "Answer 6: **LLMにおけるInference Time Scalingとは、推論時の計算資源を増やすことで、モデルの\n"
          ]
        }
      ],
      "source": [
        "print(len(answers))\n",
        "for i, answer in enumerate(answers):\n",
        "    print(f\"Answer {i + 1}: {answer[:150]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools\n",
        "from collections import defaultdict\n",
        "\n",
        "class LLMJudge:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.device = model.device\n",
        "        self.terminators = self.tokenizer.eos_token_id\n",
        "\n",
        "    def build_prompt(self, query, answer_a, answer_b):\n",
        "        \n",
        "        return [\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"\n",
        "あなたは中立で正確なAI審査員です。以下の質問と2つの回答（AとB）を比較し、どちらがより優れているかを評価してください。評価基準は、正確さ、具体性、わかりやすさです。\n",
        "質問: {query}\n",
        "\n",
        "回答 A: {answer_a}\n",
        "回答 B: {answer_b}\n",
        "\n",
        "どちらが優れていますか？「A」または「B」とだけ出力してください。\n",
        "\"\"\"}\n",
        "        ]\n",
        "        \n",
        "    def evaluate_pair(self, query, answer_a, answer_b):\n",
        "        prompt = self.build_prompt(query, answer_a, answer_b)\n",
        "        # print(f\"Prompt: {prompt}\")\n",
        "  \n",
        "        input_ids = tokenizer.apply_chat_template(\n",
        "                        prompt,\n",
        "                        add_generation_prompt=True,\n",
        "                        return_tensors=\"pt\"\n",
        "                    ).to(model.device)\n",
        "        \n",
        "        # inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "        # print(f\"Inputs: {inputs}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                input_ids,\n",
        "                eos_token_id=self.terminators,\n",
        "                max_new_tokens=32,\n",
        "                do_sample=False\n",
        "            )\n",
        "        # print(f\"Outputs: {outputs}\")\n",
        "        generated_text = self.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "        # print(f\"Generated text: {generated_text}\")\n",
        "        # 結果から判定文字列を抽出\n",
        "        if \"A\" in generated_text or \"B\" in generated_text:\n",
        "            if \"同等\" in generated_text:\n",
        "                return \"同等\"\n",
        "            elif \"A\" in generated_text.split()[0]:\n",
        "                return \"A\"\n",
        "            elif \"B\" in generated_text.split()[0]:\n",
        "                return \"B\"\n",
        "        return \"同等\"  # fallback\n",
        "\n",
        "class AnswerRanker:\n",
        "    def __init__(self, query, answers, judge: LLMJudge):\n",
        "        self.query = query\n",
        "        self.answers = answers\n",
        "        self.judge = judge\n",
        "        self.win_count = defaultdict(int)\n",
        "\n",
        "    def run_evaluation(self):\n",
        "        for i, j in itertools.combinations(range(len(self.answers)), 2):\n",
        "            answer_a, answer_b = self.answers[i], self.answers[j]\n",
        "            result = self.judge.evaluate_pair(self.query, answer_a, answer_b)\n",
        "            print(f\"Comparing #{i} vs #{j} -> {result}\")\n",
        "            if result == \"A\":\n",
        "                self.win_count[i] += 1\n",
        "            elif result == \"B\":\n",
        "                self.win_count[j] += 1\n",
        "            # 同等 → 何もしない\n",
        "\n",
        "    def get_ranking(self):\n",
        "        ranked_indices = sorted(range(len(self.answers)), key=lambda i: -self.win_count[i])\n",
        "        return [(idx, self.win_count[idx], self.answers[idx]) for idx in ranked_indices]\n",
        "\n",
        "    def print_ranking(self):\n",
        "        print(\"\\n=== RANKING ===\")\n",
        "        for rank, (idx, wins, ans) in enumerate(self.get_ranking(), start=1):\n",
        "            print(f\"{rank}. Answer #{idx} (Wins: {wins})\")\n",
        "            # print(f\"    {ans}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing #0 vs #1 -> A\n",
            "Comparing #0 vs #2 -> A\n",
            "Comparing #0 vs #3 -> A\n",
            "Comparing #0 vs #4 -> A\n",
            "Comparing #0 vs #5 -> A\n",
            "Comparing #1 vs #2 -> A\n",
            "Comparing #1 vs #3 -> A\n",
            "Comparing #1 vs #4 -> A\n",
            "Comparing #1 vs #5 -> A\n",
            "Comparing #2 vs #3 -> A\n",
            "Comparing #2 vs #4 -> B\n",
            "Comparing #2 vs #5 -> A\n",
            "Comparing #3 vs #4 -> A\n",
            "Comparing #3 vs #5 -> A\n",
            "Comparing #4 vs #5 -> A\n",
            "\n",
            "=== RANKING ===\n",
            "1. Answer #0 (Wins: 5)\n",
            "2. Answer #1 (Wins: 4)\n",
            "3. Answer #2 (Wins: 2)\n",
            "4. Answer #3 (Wins: 2)\n",
            "5. Answer #4 (Wins: 2)\n",
            "6. Answer #5 (Wins: 0)\n"
          ]
        }
      ],
      "source": [
        "query=\"[質問] LLMにおけるInference Time Scalingとは？\"\n",
        "\n",
        "judge = LLMJudge(model, tokenizer) \n",
        "ranker = AnswerRanker(query, answers, judge)\n",
        "\n",
        "ranker.run_evaluation()\n",
        "ranker.print_ranking()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
